## Notes

The "sex" variable has men listed as 1 and women listed as 0.  The plot for the "sex" variable shows us that not only are most of the data male but most of the death events are also male.

Modeling methodology: I performed grid searches for all variables (except "time) and separately for just the focus variables. My goal was to find well parameterized versions of each of the model types, not necessarily to go over the top beating the best performers in the Kaggle circuit. Again, my goal was to understand key effects to machine learning models from a two-variable and complete variable set approach.

Hierarchical clustering: these results were fascinating to me. It's probably important for the two-variable model performance that ejection fraction and serum creatinine do not share a common cluster at all.  Additionally, if you Google creatinine phosphokinase and diabetes (a clustering pairing), you will find studies linking the two -- one of which can be found here.  Finally, ejection fraction is ascertained through imaging techniques, while serum sodium is measured through a blood sample. For areas of the world where imaging a patient is not possible, but blood samples are, it would interesting if machine learning could use serum sodium instead of ejection fraction in a two-variable model to predict heart failure.

F1 Scores Barplot: This plot is one of the center points of my analysis as it strongly supports the researchers' findings in one plot and makes my hypothesis less plausible. Recall that my hypothesis was that their two variable model would have higher scoring variation across cross validation splits.  Each column in the plot represents the F1 scores for 5 randomly selected samples of the training data. The ideal result would be a very short boxplot located at or near the top of the grid. Across the models the "foc" variable set (i.e., ejection fraction and serum creatinine variables only) have less variation in the Logistic Regression, KNN, and Random Forest models. In the cases of the SVC and Gradient Boosted Classifier (GBC) models, the marginally higher variation is eclipsed by both higher median F1 scores and a distribution of scores across cross validation that mostly beat the all variable model.

Accuracy Barplot: The message is more mixed when we evaluate the same plot for accuracy. Zooming in on the two best models, Random Forest and GBC, shows the case where the all variable model should be preferred as it has roughly the same variation as the focus variable model, but with a higher median score. Alternatively, GBC produced the same median scores across variable sets, but with the key difference that the higher variation in the two-model approach meant some cases where results were significantly better.

Probability line plot: This plot shows the models' probabilities associated with their prediction on the x-axis and the accuracy of their prediction on the y-axis. The idea here is that if the model tells warns the physician of a death event with a probability score of 80%, we would expect the accuracy of that prediction to be 80% in the long-run. The gray line in the plot shows this theoretical connection between the x and y axes. Ideally, the "All" and "Focus" lines would stay very close to the gray line and on occasions where it misses, be above the gray line (indicating a higher accuracy than might have been hoped for given a stated probability).  The blue and red lines each include all of the machine learning types for their respective set of variables.  While up for debate, this plot tells me that while the F1 boxplot earlier might have championed two-variable models, physicians should be wary of the probability scores such an approach offers them. One area of particular concern is for probabilities exceeding 80% where the two-variable models do not gain any more accuracy despite increasing their probability scores. Said another way, if a two-variable model tells you its prediction with a 95% probability, you should assume it's only about 80% accurate.  The All variable model shines a little more brightly in this regard in that it hugs the gray line a little more closely (for the most part). In the 83% - 89% probability regions its actual accuracy is just as disappointing as the two-variable model (~77%), but for probabilities above that its accuracy actually improves, whereas the two-variable models do not.

Statistical Significance - Monte Carlo (All Variable Model): This approach to significance testing takes the predictions made by each model, shuffles them using a permuted congruential generator that randomly permutes the values using samples from different distributions and records the accuracy and F1 scores. This is performed 5,000 times for each model. The code then finds the 95th percentile accuracy and F1 scores and inserts it into the dataframe. The main goal here is to avoid inferring a distribution to the dataset while testing the likelihood that a lucky guess from the model could have produced the same accuracy and F1 scores at the 95th percentile level.  The results of this exercise reveal that the logistic regression, random forest, and gradient boosted classifer have statistically significant results in the test sample at the 95th percentile for both accuracy and F1 in the case of the all variable model. The support vector classifier and knn fall short on both accuracy and F1.  The results also show that in the test set then random forest and gradient boosted classifier models achieve accuracy scores of 77.8% and F1 scores of 64.3% and 61.5%, respectively.

Statistical Significance - Monte Carlo (Two Variable Model): There are a few alarming things in these results, first and foremost of which is the large drop in accuracy and F1 scores on the test dataset for the two-variable model. This suggests that while the two-variable model performed well in cross-validation, it did not end up generalizing well to the test data -- possibly the result of a greater suspectibility the two factor model has to overfitting than the model with more variables does.  The support vector classifier ends up being the only statistically significant model in the two-variable set-up. All other models had results at what the 95th percentile of lucky/random guesses would have achieved.


## Conclusion

The study's claim that ejection fraction and serum creatinine alone produced not only adequate but actually better machine learning models to predict heart failure appears to have some important flaws. First, its ability to correctly guess probability of heart failure stops appropriately progressing past the 80% probability level (an issue the all variable model has less trouble with) and, second, the two-variable models struggles to beat the 95th percentile guesses the Monte Carlo simulation produced in all cases except the SVC model.

On the training data, the two-variable model shines in cross validation and produces better F1 scores but fails to produce superior accuracy scores.

My best model overall is the all variable random forest model, which produced a 77.8% accuracy score and 64.3% F1 score in testing. Notwithstanding the results of the Monte Carlo simulation, none of the two variable models produced that level of performance against the test data.

In summary, if we consider the two-variable model's superior F1 results in cross-validation offset by in the inferior ones in accuracy testing and probability performance, then the inability to frequently beat the 95th percentile result of a Monte Carlo simulation and inferior outright results against the test data make the two variable model a worse alternative to the all variable model for physician use.
